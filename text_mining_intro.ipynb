{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You are currently looking at **version 1.0** of this notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining areas in short:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Working text\n",
    "Working with text needs a tool box that is quite different from working with numerical data. Generally characters, words, sentences need to be cleaned and (pre)processed before doing actual analyses. Luckily their are very valuable frameworks and toolboxes around, like NLTK:\n",
    " - NLTK documentation link: http://www.nltk.org/api/nltk.html\n",
    " - NLTK cheat sheet: https://blogs.princeton.edu/etc/files/2014/03/Text-Analysis-with-NLTK-Cheatsheet.pdf\n",
    " - NLTK book: http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis\n",
    "Sentiment analysis is generally a starting point in analyzing a text and is then coupled with other techniques (e.g., topic analysis). Sentiment analysis is usually done using a corpus of positive and negative words.\n",
    "It identifies entities and emotions in a sentence and use these to determine if the entity is being viewed positively or negatively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easy example sentiment analyses\n",
    "<li>I had an <b style=\"color:green\">excellent</b> souffle at the restaurant Cavity Maker</li>\n",
    "<li>Excellent is a positive word for both the souffle as well as for the restaurant</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not so easy examples\n",
    "Often, looking at words alone is not enough to figure out the sentiment:  \n",
    "<li><i>The Girl on the Train is an <span style=\"color:green\">excellent</span> book for a ‘stuck at home’ snow day</i></li> This one is easy since it includes an explicit positive opinion using a positive word\n",
    "<li><i>The Girl on the Train is an <span style=\"color:green\">excellent</span> book for using as a liner for your cat’s litter box</i></li> Not so simple! The positive word \"excellent\" is used with a negative connotation. \n",
    "<li><i>The Girl on the Train is <span style=\"color:green\">better</span> than Gone Girl</i></li> The positive word is used as a comparator. Whether the writer likes The Girl on the Train or not depends on what he or she thinks of Gone Girl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources of sentiment coded words\n",
    "<ol>\n",
    "<li>Hu and Liu's sentiment analysis lexicon: words coded as either positive or negative</li>\n",
    "<ul>\n",
    "<li>http://ptrckprry.com/course/ssd/data/positive-words.txt\n",
    "<li>http://ptrckprry.com/course/ssd/data/negative-words.txt\n",
    "</ul>\n",
    "<li>NRC Emotion Lexicon: words coded into emotional categories (many languages)</li>\n",
    "<ul>\n",
    "<li>http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm</li>\n",
    "</ul>\n",
    "<li>SentiWordNet: Lists of words weighted by positive or negative sentiment. Includes guidance on how to use the words</li>\n",
    "<ul>\n",
    "<li>http://sentiwordnet.isti.cnr.it/</li>\n",
    "</ul>\n",
    "<li>Vadar Sentiment tool: 7800 words with positive or negative polarity</li>\n",
    "<ul>\n",
    "<li>Included with python nltk</li>\n",
    "</ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topic modeling\n",
    "The goal of topic modeling is to identify the major concepts underlying a piece of text.  \n",
    "Topic modeling uses \"Unsupervised Learning\". No apriori knowledge is necessary.  \n",
    "Though it is helpful in cleaning up results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup notebook\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the generic libraries used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from collections import OrderedDict, Counter\n",
    "import pprint\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set defaults and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas defaults\n",
    "pd.set_option('max_rows', 10)                                # Show max 10 rows: head(5) ... tail(5)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)  # Set precision of DataFrames/Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check current working directory and file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Working text\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Ethics are built right into the ideals and objectives of the United Nations \"\n",
    "n_chars = len(text1) # The length of text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = text1.split(' ') # Return a list of the words in text2, separating by ' '.\n",
    "n_words = len(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text2)\n",
    "n_chars, n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list('abcdefghijklm'), list('1234567890')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List comprehension allows us to find specific words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in text2 if len(w) > 3] # Words that are greater than 3 letters long in text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in text2 if w.istitle()] # Capitalized words in text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in text2 if w.endswith('s')] # Words in text2 that end in 's'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We can find unique words using `set()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = 'To be or not to be'\n",
    "text4 = text3.split(' ')\n",
    "len(text4), len(set(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([w.lower() for w in text4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing free-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = '\"Ethics are built right into the ideals and objectives of the United Nations\" \\\n",
    "#UNSG @ NY Society for Ethical Culture bit.ly/2guVelr'\n",
    "text6 = text5.split(' ')\n",
    "text6;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding hastags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in text6 if w.startswith('#')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding callouts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in text6 if w.startswith('@')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text7 = '@UN @UN_Women \"Ethics are built right into the ideals and objectives of the United Nations\" \\\n",
    "#UNSG @ NY Society for Ethical Culture bit.ly/2guVelr'\n",
    "text8 = text7.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular expressions help us with more complex parsing\n",
    "For example `'@[A-Za-z0-9_]+'` will return all words that: \n",
    "* start with `'@'` and are followed by at least one: \n",
    "* capital letter (`'A-Z'`)\n",
    "* lowercase letter (`'a-z'`) \n",
    "* number (`'0-9'`)\n",
    "* or underscore (`'_'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in text8 if re.search('@[A-Za-z0-9_]+', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a labeled data set; [(text, label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/sentiment_labelled_sentences/full_set.txt\") as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First look at data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split sentences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove leading and trailing white spaces before splitting labels\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "## Separate the sentences from the labels; '\\t1\\n' => 1 is the label\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text data\n",
    "\n",
    "To transform this prediction problem into one amenable to linear classification, we will first need to preprocess the text data. We will do four transformations:\n",
    "\n",
    "1. Remove punctuation and numbers.\n",
    "2. Transform all words to lower-case.\n",
    "3. Remove _stop words_.\n",
    "4. Convert the sentences into vectors, using a bag-of-words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_remove(x, removal_list):\n",
    "    # Replace chars from removal list with spaces\n",
    "    for remove_item in removal_list:\n",
    "        x = x.replace(remove_item, ' ')\n",
    "    # Return without superfluous spaces\n",
    "    return ' '.join(x.split(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove digits\n",
    "digit_less = [full_remove(x, list('1234567890')) for x in sentences]\n",
    "\n",
    "## Remove punctuation\n",
    "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
    "\n",
    "## Make everything lower-case\n",
    "sents_lower = [x.lower() for x in punc_less]\n",
    "type(sents_lower), sents_lower[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words\n",
    " - Stop words are words that are filtered out because they are believed to contain no useful information for the task at hand. You can create your own arbitrary stop word list or use a generic one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ' '.join([sent_words for sent_words in sents_lower])\n",
    "dictionary = set(corpus.split())\n",
    "\n",
    "# Use predefined stop words set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define our own unwanted words set\n",
    "unwanted_words = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "# Get short words\n",
    "MIN_LENGTH = 3\n",
    "short_words = set([word for word in dictionary if len(word) < MIN_LENGTH])\n",
    "\n",
    "# Define set of words to clear from text/sentences\n",
    "clear_set = stop_words | unwanted_words | short_words\n",
    "\n",
    "# Clear text from unwanted words\n",
    "sents_split = [x.split() for x in sents_lower]\n",
    "sents_processed = [' '.join(list(filter(lambda word: word not in clear_set, sent_words))) for sent_words in sents_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the sentences look like so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_processed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Basic NLP Tasks with NLTK\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK sources\n",
    " - ntlk documentation link: http://www.nltk.org/api/nltk.html\n",
    " - Commands cheat sheet: https://blogs.princeton.edu/etc/files/2014/03/Text-Analysis-with-NLTK-Cheatsheet.pdf\n",
    " - nltk book: http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'no words in text:', len(text7), text7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'no words in sentence:', len(sent7), sent7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'no unique words:', len(set(text7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'first 10 unique words:', list(set(text7))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = FreqDist(text7)\n",
    "dist2 = Counter(text7)\n",
    "len(dist), dist == dist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1 = dist.keys()\n",
    "# vocab1[:10] # can't slice in python 3\n",
    "\n",
    "# Python 3 dict.keys() returns an iterable view instead of a list\n",
    "list(vocab1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'frequency of key in text:', dist['four']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqwords = [w for w in vocab1 if len(w) > 5 and dist[w] > 100]\n",
    "'words with more than 5 characters and frequency higher than 100:', freqwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and stemming\n",
    "Stemming is the process for reducing inflected/derived words to their stem/base/root. The stem need not be identical to the morphological root of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = 'List listed lists listing listings'\n",
    "words1 = input1.lower().split(' ')\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in words1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatisation is the process of grouping together the different inflected forms.\n",
    "For example, in English, the verb ‘to walk’ may appear as ‘walk’, ‘walked’, ‘walks’, ‘walking’. The base form, ‘walk’, that one might look up in a dictionary, is called the lemma for the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputl = 'Walk walked walks walking walker Walkers'\n",
    "wordsl = inputl.lower().split(' ')\n",
    "\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "'walks => walk ', [WNlemma.lemmatize(t) for t in wordsl], [WNlemma.lemmatize(t) for t in wordsl] == wordsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "'Universal declaration of human rights corpus:', udhr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[porter.stem(t) for t in udhr[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "lemmatized = [WNlemma.lemmatize(t) for t in udhr[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(lemmatized)) / len(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "text11.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = nltk.word_tokenize(text11)\n",
    "text_nltk = nltk.Text(text_tokens)\n",
    "text_tokens, text_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(text11), '-'*50, 'no of words:', len(nltk.word_tokenize(text11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "sentences = nltk.sent_tokenize(text12)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nltk.word_tokenize(moby_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1[:10], nltk.Text(text1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(text1).lower().split(' ')\n",
    "dist = FreqDist(words)#.most_common() in ['whale', 'Whale']\n",
    "dist['whale'] * 100 / len(nltk.word_tokenize(' '.join(text1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreqDist(text1).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word length > 5, frequency > 150\n",
    "dist = FreqDist(text1).most_common()\n",
    "sorted([k for k, v in dist if len(k) > 5 and v > 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longest word + length\n",
    "from collections import OrderedDict\n",
    "dist = FreqDist(text1).most_common()\n",
    "\n",
    "# dictionary sorted by length of the key string\n",
    "longest_word = OrderedDict(sorted(dist, key=lambda t: len(t[0]), reverse=True)).popitem(last=False)\n",
    "longest_word[0], len(longest_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series({len(w):w for w in text1})[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique words with frequency of more than 2000 and their frequency\n",
    "dist = FreqDist(text1).most_common(50)\n",
    "result = sorted([(f, w) for w, f in dist if f > 2000 and w.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average # tokens per sentance\n",
    "sentences = nltk.sent_tokenize(' '.join(text1))\n",
    "np.mean([len(nltk.word_tokenize(s)) for s in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Advanced NLP Tasks with NLTK\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset('NN'), nltk.help.upenn_tagset('DT'), nltk.help.upenn_tagset('VB'), nltk.help.upenn_tagset('MD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text13 = nltk.word_tokenize(text11)\n",
    "nltk.pos_tag(text13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text14 = nltk.word_tokenize(\"Visiting aunts can be a nuisance\")\n",
    "nltk.pos_tag(text14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing sentence structure\n",
    "text15 = nltk.word_tokenize(\"Alice loves Bob\")\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP\n",
    "NP -> 'Alice' | 'Bob'\n",
    "V -> 'loves'\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "trees = parser.parse_all(text15)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "text17 = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "print(text17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging and parsing ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text18 = nltk.word_tokenize(\"The old man the boat\")\n",
    "nltk.pos_tag(text18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text19 = nltk.word_tokenize(\"Colorless green ideas sleep furiously\")\n",
    "nltk.pos_tag(text19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities: People, places, organizations\n",
    " - Named entities are often the subject of sentiments so identifying them can be very useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity detection - Part-of-speech tagging\n",
    " - tokenize sentences with sentence detector (english)\n",
    " - tokenize words in each sentence\n",
    " - chunk them; ne_chunk identifies likely chunked candidates (ne = named entity)\n",
    " - build chunks using nltk's guess on what members of chunk represent (people, place, organization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en={}\n",
    "try:\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(community_data.raw().strip())\n",
    "    for sentence in sentences:\n",
    "            tokenized = nltk.word_tokenize(sentence)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            chunked = nltk.ne_chunk(tagged)\n",
    "            for tree in chunked:\n",
    "                if hasattr(tree, 'label'):\n",
    "                    ne = ' '.join(c[0] for c in tree.leaves())\n",
    "                    en[ne] = [tree.label(), ' '.join(c[1] for c in tree.leaves())]\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent parts of speech in this text? What is their frequency?\n",
    "df = pd.DataFrame(nltk.pos_tag(text1))\n",
    "df.columns = ['word', 'pos']\n",
    "df = df.groupby('pos')['pos'].count().sort_values(ascending=False)\n",
    "list(zip(df.head(5).index, df.head(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Text Mining areas explained\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sentiment Analysis\n",
    "Sentiment analysis is generally a starting point in analyzing a text and is then coupled with other techniques (e.g., topic analysis). Sentiment analysis is usually done using a corpus of positive and negative words.\n",
    "It identifies entities and emotions in a sentence and use these to determine if the entity is being viewed positively or negatively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easy example sentiment analyses\n",
    "<li>I had an <b style=\"color:green\">excellent</b> souffle at the restaurant Cavity Maker</li>\n",
    "<li>Excellent is a positive word for both the souffle as well as for the restaurant</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not so easy examples\n",
    "Often, looking at words alone is not enough to figure out the sentiment:  \n",
    "<li><i>The Girl on the Train is an <span style=\"color:green\">excellent</span> book for a ‘stuck at home’ snow day</i></li> This one is easy since it includes an explicit positive opinion using a positive word\n",
    "<li><i>The Girl on the Train is an <span style=\"color:green\">excellent</span> book for using as a liner for your cat’s litter box</i></li> Not so simple! The positive word \"excellent\" is used with a negative connotation. \n",
    "<li><i>The Girl on the Train is <span style=\"color:green\">better</span> than Gone Girl</i></li> The positive word is used as a comparator. Whether the writer likes The Girl on the Train or not depends on what he or she thinks of Gone Girl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources of sentiment coded words\n",
    "<ol>\n",
    "<li>Hu and Liu's sentiment analysis lexicon: words coded as either positive or negative</li>\n",
    "<ul>\n",
    "<li>http://ptrckprry.com/course/ssd/data/positive-words.txt\n",
    "<li>http://ptrckprry.com/course/ssd/data/negative-words.txt\n",
    "</ul>\n",
    "<li>NRC Emotion Lexicon: words coded into emotional categories (many languages)</li>\n",
    "<ul>\n",
    "<li>http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm</li>\n",
    "</ul>\n",
    "<li>SentiWordNet: Lists of words weighted by positive or negative sentiment. Includes guidance on how to use the words</li>\n",
    "<ul>\n",
    "<li>http://sentiwordnet.isti.cnr.it/</li>\n",
    "</ul>\n",
    "<li>Vadar Sentiment tool: 7800 words with positive or negative polarity</li>\n",
    "<ul>\n",
    "<li>Included with python nltk</li>\n",
    "</ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple sentiment analysis using from Hu and Liu's sentiment analysis lexicon\n",
    "Compute the proportion of positive and negative words in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download() # download datasets = corpi\n",
    "from nltk import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords, inaugural, PlaintextCorpusReader\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get positive and negative words from Hu and Liu's sentiment analysis lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(url):\n",
    "    words = requests.get(url).content.decode('latin-1')\n",
    "    word_list = words.split('\\n')\n",
    "    words_list = [word for word in word_list if ';' not in word if word != '']\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of positive and negative words\n",
    "p_url = 'http://ptrckprry.com/course/ssd/data/positive-words.txt'\n",
    "n_url = 'http://ptrckprry.com/course/ssd/data/negative-words.txt'\n",
    "positive_words = get_words(p_url)\n",
    "negative_words = get_words(n_url)\n",
    "positive_words[:5], negative_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/community.txt','r') as f:\n",
    "    community = f.read()\n",
    "with open('data/le_monde.txt','r') as f:\n",
    "    le_monde = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique words in positive and negative domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('community:', len(set(word_tokenize(community)) & set(positive_words))*100/len(set(word_tokenize(community))), \n",
    "len(set(word_tokenize(community)) & set(negative_words))*100/len(set(word_tokenize(community))), '\\nle monde:',\n",
    "      \n",
    "len(set(word_tokenize(le_monde)) & set(positive_words))*100/len(set(word_tokenize(le_monde))), \n",
    "len(set(word_tokenize(le_monde)) & set(negative_words))*100/len(set(word_tokenize(le_monde))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative frequency words in positive and negative domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpos, cneg, lpos, lneg = 0, 0, 0, 0\n",
    "\n",
    "ttl_community = len(word_tokenize(community))\n",
    "ttl_le_monde = len(word_tokenize(le_monde))\n",
    "\n",
    "for word in word_tokenize(community):\n",
    "    if word in positive_words:\n",
    "        cpos += 100/ttl_community\n",
    "    if word in negative_words:\n",
    "        cneg += 100/ttl_community\n",
    "        \n",
    "for word in word_tokenize(le_monde):\n",
    "    if word in positive_words:\n",
    "        lpos += 100/ttl_le_monde\n",
    "    if word in negative_words:\n",
    "        lneg += 100/ttl_le_monde\n",
    "\n",
    "print(\"text source: {}\\t {}\\t {}\".format('positive','negative','difference'))    \n",
    "print(\"community:   {0:1.2f}\\t {1:1.2f}\\t\\t {2:1.2f}\".format(cpos, cneg, (cpos-cneg)))\n",
    "print(\"le monde:    {0:1.2f}\\t {1:1.2f}\\t\\t {2:1.2f}\".format(lpos, lneg, (lpos-lneg)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple sentiment analysis using a associations with emotions \n",
    "- 8 emoitions: anger, fear, anticipation, trust, surprise, sadness, joy, and disgust\n",
    "- NRC data codifies words with emotions\n",
    "- 14,182 words are coded into 2 sentiments and 8 emotions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For example, the word abandonment is associated with anger, fear, sadness and has a negative sentiment\n",
    "- abandoned\tanger\t        1\n",
    "- abandoned\tanticipation\t0\n",
    "- abandoned\tdisgust\t        0\n",
    "- abandoned\tfear\t        1\n",
    "- abandoned\tjoy\t            0\n",
    "- abandoned\tnegative\t    1\n",
    "- abandoned\tpositive\t    0\n",
    "- abandoned\tsadness\t        1\n",
    "- abandoned\tsurprise\t    0\n",
    "- abandoned\ttrust\t        0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build dictionary of the NRC sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -50 data/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nrc_data(nrc):\n",
    "    count = 0\n",
    "    emotion_dict = {}\n",
    "    with open(nrc, 'r') as f:\n",
    "        all_lines = []\n",
    "        for line in f:\n",
    "            # skip first 46 lines\n",
    "            if count < 46: \n",
    "                count += 1\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            # add associations to dict\n",
    "            if int(line[2]) == 1:\n",
    "                # build list of assciations (within dict) for each (key)word\n",
    "                if emotion_dict.get(line[0]):\n",
    "                    emotion_dict[line[0]].append(line[1])\n",
    "                else:\n",
    "                    emotion_dict[line[0]] = [line[1]]\n",
    "    return emotion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = get_nrc_data(\"data/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\")\n",
    "emotion_dict['abandoned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YELP without API\n",
    " - make sure you've the file \"yelp_data.pickle\" in the same directory as your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('data/yelp_data.pickle', 'rb') as fp:\n",
    "    all_snippets = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "all_snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp with API\n",
    "- https://www.yelp.com/developers/documentation/v3\n",
    "- log into yelp (top right hand corner of the page)\n",
    "- Click <span style=\"color:blue\">Create App</span> on the left hand menu bar\n",
    "- Enter app info (leave optional stuff blank)\n",
    "- Copy the client id and API key to a secure place (this notebook should do the trick or use a text file!)\n",
    "- You might need to generate the API key by clicking on Manage App after creating an App. Make sure you get one!\n",
    "- Safe credentials to file and set permissions CHMOD 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read keys from credentials file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yelp_credentials.txt') as f:\n",
    "    contents = f.read().split('\\n')\n",
    "    CLIENT_ID = contents[0]\n",
    "    API_KEY = contents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API constants, you shouldn't have to change these.\n",
    "API_KEY\n",
    "API_HOST = 'https://api.yelp.com'     # The API url header\n",
    "BUSINESS_PATH = '/v3/businesses/'     # The path to get data for a single business\n",
    "SEARCH_PATH = '/v3/businesses/search' # The path for an API request to find businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(business_id, search=False, search_data=''):\n",
    "    # Construct url for purpose\n",
    "    if search:\n",
    "        url = API_HOST + SEARCH_PATH\n",
    "    else:\n",
    "        url = '{}{}{}/reviews'.format(API_HOST, BUSINESS_PATH, business_id)\n",
    "    # Authorisation\n",
    "    headers = {'Authorization': 'Bearer {}'.format(API_KEY)}\n",
    "    response = requests.get(url, headers=headers).json()\n",
    "    return requests.get(url, headers=headers, params=search_data).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business(business, location, number=15):\n",
    "    search_data = {'term': business, 'location': location.replace(' ', '+'), 'limit': number}\n",
    "    return get_response(business, True, search_data)['businesses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'Amsterdam, Netherlands'\n",
    "business = 'restaurant'\n",
    "get_business(business, location, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'Amsterdam, Netherlands'\n",
    "df = pd.DataFrame(get_business(business, location, 20))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_review(business_id):\n",
    "    response = get_response(business_id)\n",
    "    return [review['text'] for review in response['reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_business_review(df['alias'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(business, location, number=25):\n",
    "    businesses = get_business(business, location, number)\n",
    "    if businesses:\n",
    "        review_list = []\n",
    "        for business in businesses:\n",
    "            review_text = get_business_review(business['alias'])\n",
    "            review_list.append((business['alias'], business['name'], review_text))\n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_snippets = get_reviews(business, location)\n",
    "all_snippets[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze emotions in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_analyser(text, emotion_dict=emotion_dict):\n",
    "    '''Compute percentage of words associated with emotion'''\n",
    "    # Set up the emotion count dict\n",
    "    emotions = {x for y in emotion_dict.values() for x in y}\n",
    "    emotion_count = {}\n",
    "    for emotion in emotions:\n",
    "        emotion_count[emotion] = 0\n",
    "\n",
    "    # Count emotions and normalize by total number of words\n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "    for word in words:\n",
    "        if emotion_dict.get(word):\n",
    "            for emotion in emotion_dict.get(word):\n",
    "                emotion_count[emotion] += 1/total_words*100\n",
    "                \n",
    "    return emotion_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse joint reviews for each restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyser(snippets, emotion_dict=emotion_dict):\n",
    "    df_ = pd.DataFrame([emotion_analyser(' '.join(snippet[2]), emotion_dict) for snippet in snippets])\n",
    "    df_['restaurant'] = [snippet[1] for snippet in snippets]\n",
    "    df_.set_index('restaurant', inplace=True)\n",
    "    pd.set_option('display.float_format', lambda x: '{:.1f}%'.format(x))\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyser(all_snippets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine address, reviews and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_nearby_restaurants(business, address, number=15):\n",
    "    snippets = get_reviews(business, address, number)\n",
    "    return sentiment_analyser(snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_nearby_restaurants('bar', 'Community Food and Juice, New York, NY', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_nearby_restaurants('pub', 'Amsterdam', 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(snippets):\n",
    "    'Combine all reviews to one corpus and generate wordcloud'\n",
    "    text = ' '.join(str(snippet[2]) for snippet in snippets)\n",
    "    return WordCloud(stopwords=STOPWORDS, background_color='white', width=3000, height=3000).generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(word_cloud(all_snippets))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_dict = {snippet[1]:' '.join(snippet[2]) for snippet in all_snippets}\n",
    "reviews_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = reviews_dict['Cannibale Royale']\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', width=3000, height=3000).generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the PlainTextCorpusReader to combine text files into corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community = \"data/community\", \"community.*\"\n",
    "le_monde = \"data/le_monde\", \"le_monde.*\"\n",
    "heights = \"data/heights\", \"heights.*\"\n",
    "amigos = \"data/amigos\", \"amigos.*\"\n",
    "\n",
    "community_data = PlaintextCorpusReader(*community)\n",
    "le_monde_data = PlaintextCorpusReader(*le_monde)\n",
    "heights_data = PlaintextCorpusReader(*heights)\n",
    "amigos_data = PlaintextCorpusReader(*amigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amigos_data.fileids()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amigos_data.raw()[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified sentiment analyser\n",
    " - snippets are joint in raw file\n",
    " - tuple consist of (resto, review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyser_2(snippets, emotion_dict=emotion_dict):\n",
    "    df_ = pd.DataFrame([emotion_analyser(snippet[1]) for snippet in snippets])\n",
    "    df_['restaurant'] = [snippet[0] for snippet in snippets]\n",
    "    df_.set_index('restaurant', inplace=True)\n",
    "    pd.set_option('display.float_format', lambda x: '{:.1f}%'.format(x))\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_analyser(community_data.raw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_data = [('community', community_data.raw()),('le monde', le_monde_data.raw())\n",
    "                  ,('heights', heights_data.raw()), ('amigos', amigos_data.raw())]\n",
    "\n",
    "sentiment_analyser_2(restaurant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Complexity analysis\n",
    "complexity factors:\n",
    " - average word length: longer words adds to complexity\n",
    " - average sentence length: longer sentences are more complex (unless the text is rambling!)\n",
    " - vocabulary: the ratio of unique words used to the total number of words (more variety, more complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>token:</b> A sequence (or group) of characters of interest. \n",
    "For e.g., in the below analysis, a token = a word\n",
    " - A token is the base unit of analysis\n",
    " - convert text into tokens and nltk text object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = le_monde_data.raw()\n",
    "sentences = nltk.Text(sent_tokenize(text))\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.Text(word_tokenize(text))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complexity(text):\n",
    "    num_chars = len(text)\n",
    "    num_words = len(word_tokenize(text))\n",
    "    num_sentences = len(sent_tokenize(text))\n",
    "    vocab = {x.lower() for x in word_tokenize(text)}\n",
    "    return len(vocab),int(num_chars/num_words),int(num_words/num_sentences),len(vocab)/num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_complexity(le_monde_data.raw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i, text in enumerate(restaurant_data):\n",
    "    (vocab, word_size, sent_size, vocab_to_text) = get_complexity(text[1])\n",
    "    df.loc[i, 'text'] = text[0]\n",
    "    df.loc[i, 'vocab_size'] = vocab\n",
    "    df.loc[i, 'word_size'] = word_size\n",
    "    df.loc[i, 'sent_size'] = sent_size\n",
    "    df.loc[i, 'unique_words'] = vocab_to_text\n",
    "    pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word cloud comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = restaurant_data\n",
    "\n",
    "# Remove unwanted words\n",
    "DELETE_WORDS = []\n",
    "\n",
    "def remove_words(text_string, DELETE_WORDS=DELETE_WORDS):\n",
    "    for word in DELETE_WORDS:\n",
    "        text_string = text_string.replace(word,' ')\n",
    "    return text_string\n",
    "\n",
    "# Remove short words\n",
    "MIN_LENGTH = 4\n",
    "\n",
    "def remove_short_words(text_string, min_length=MIN_LENGTH):\n",
    "    word_list = text_string.split()\n",
    "    for word in word_list:\n",
    "        if len(word) < min_length:\n",
    "            text_string = text_string.replace(' '+word+' ', ' ', 1)\n",
    "    return text_string\n",
    "\n",
    "\n",
    "# Setup axes/grid\n",
    "ROW_NUM, COL_NUM = 2, 2\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(12, 12))\n",
    "\n",
    "for i in range(0, len(texts)):\n",
    "    text_string = remove_words(texts[i][1])\n",
    "    text_string = remove_short_words(text_string)\n",
    "    ax = axes[i%2]\n",
    "    ax = axes[i//2, i%2] # Use this if ROW_NUM >=2\n",
    "    ax.set_title(texts[i][0])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', width=1200, height=1000, max_words=20).generate(text_string)\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk contains a large corpora of pre-tokenized text \n",
    "- use nltk.download() to import the corpora\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparitive analysis\n",
    "- compare US President inaugural speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.fileids()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.raw('2017-Trump.txt')[:220]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's look at the complexity of the speeches by four presidents</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [('trump', inaugural.raw('2017-Trump.txt')),\n",
    "         ('obama', inaugural.raw('2009-Obama.txt')+inaugural.raw('2013-Obama.txt')),\n",
    "         ('jackson', inaugural.raw('1829-Jackson.txt')+inaugural.raw('1833-Jackson.txt')),\n",
    "         ('washington', inaugural.raw('1789-Washington.txt')+inaugural.raw('1793-Washington.txt'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([get_complexity(text[1]) for text in texts], \n",
    "             index=['Trump', 'Obama', 'Jackson', 'Washington'],\n",
    "             columns=['vocab', 'word_size', 'sent_size', 'vocab_to_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence length over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year, sentence_lengths = [], []\n",
    "for fileid in inaugural.fileids():\n",
    "    year.append(fileid.split('-')[0])\n",
    "    sentence_lengths.append(get_complexity(' '.join(inaugural.words(fileid)))[2])\n",
    "plt.figure(figsize=(8,12))\n",
    "plt.plot(sentence_lengths, year);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion plots \n",
    " - show the relative frequency and location of words over the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "text4.dispersion_plot(['government', 'citizen', 'freedom', 'duties', 'America', 'independence', 'God', 'patriotism'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word stemmming\n",
    " - example: patriot, patriotic, patriotism all express roughly the same idea\n",
    " - nltk has a stemmer that implements the \"Porter Stemming Algorithm\" (https://tartarus.org/martin/PorterStemmer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_text(raw):\n",
    "    striptext = raw.replace('\\n\\n', ' ')\n",
    "    return striptext.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "striptext = strip_text(inaugural.raw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(striptext)\n",
    "words = word_tokenize(striptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "text = nltk.Text([p_stemmer.stem(i).lower() for i in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counter = Counter(text)\n",
    "# word_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "text.dispersion_plot(['govern', 'citizen', 'free', 'america', 'independ', 'god', 'patriot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted word analysis using Vader\n",
    "- Vader contains a list of 7500 features weighted by how positive or negative they are\n",
    "- It uses these features to calculate stats on how positive, negative and neutral a passage is\n",
    "- And combines these results to give a compound sentiment (higher = more positive) for the passage\n",
    "- Human trained on twitter-data and generally considered good for informal communication\n",
    "- 10 humans rated each feature in each tweet in context from -4 to +4\n",
    "- Calculates the sentiment in a sentence using word order analysis\n",
    "- \"marginally good\" will get a lower positive score than \"extremely good\"\n",
    "- Computes a \"compound\" score based on heuristics (between -1 and +1)\n",
    "- Includes sentiment of emoticons, punctuation, and other 'social media' lexicon elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_comparison(texts):\n",
    "    import numpy as np\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    restos = [resto[0] for resto in restaurant_data]\n",
    "    df_ = pd.DataFrame(np.zeros(4*len(restos)).reshape(-1, 4), \n",
    "                       index=restos,\n",
    "                       columns=['positive', 'negative', 'neutral', 'compound'])\n",
    "    \n",
    "    for resto in restaurant_data:\n",
    "        name = resto[0]\n",
    "        sentences = sent_tokenize(resto[1])\n",
    "        n = len(sentences)\n",
    "        for sentence in sentences:\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            df_.loc[name, 'positive'] += vs['pos']/n\n",
    "            df_.loc[name, 'negative'] += vs['neg']/n\n",
    "            df_.loc[name, 'neutral'] += vs['neu']/n\n",
    "            df_.loc[name, 'compound'] += vs['compound']/n\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_comparison(restaurant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get text sentiment when 'service' is mentioned in sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_sents = [(i, sentence) for i, sentence in enumerate(sentences) if 'service' in sentence]\n",
    "vader_comparison(meaningful_sents)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Text summaration\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affect calculator for common terms in our domain (e.g., food items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affect(text, keyword, lower=False):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(text.strip())\n",
    "    sentence_count, running_total = 0, 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if lower: sentence = sentence.lower()\n",
    "        if keyword in sentence:\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            running_total += vs['compound']\n",
    "            sentence_count += 1\n",
    "    if sentence_count == 0: return 0\n",
    "    return running_total/sentence_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_affect(community_data.raw(), 'service', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance returns text fragments around a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.Text(community_data.words()).concordance('service', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarisation\n",
    " - generate a short summary of a large piece of text automatically\n",
    " - use these summaries as input into a topic analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A naive form of summarization \n",
    "- identify the most frequent words in a piece of text\n",
    "- use the occurrence of these words in sentences to rate the importance of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sentences, candidate_sentences, candidate_sentence_counts = [], {}, {}\n",
    "striptext = strip_text(community_data.raw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unimportant, stopwords and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(striptext)\n",
    "lowercase_words = [word.lower() for word in words\n",
    "                  if word not in stopwords.words() and word.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = Counter(lowercase_words)\n",
    "most_frequent_words = word_frequencies.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(most_frequent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase the sentences\n",
    "candidate_sentences is a dictionary with the original sentence as the key, and its lowercase version as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_sentences = {sentence: sentence.lower() for sentence in sent_tokenize(striptext)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarise word frequency scores per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for long, short in candidate_sentences.items():\n",
    "    count = 0\n",
    "    for freq_word, frequency_score in most_frequent_words:\n",
    "        if freq_word in short:\n",
    "            count += frequency_score\n",
    "            candidate_sentence_counts[long] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentences = OrderedDict(sorted(\n",
    "                    candidate_sentence_counts.items(),\n",
    "                    key = lambda x: x[0],\n",
    "                    reverse = True)[:4])\n",
    "pp.pprint(sorted_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Packaging all this into a function</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_naive_summary(text):\n",
    "\n",
    "    striptext = text.replace('\\n\\n', ' ')\n",
    "    striptext = striptext.replace('\\n', ' ')\n",
    "\n",
    "    lowercase_words = [word.lower() for word in word_tokenize(striptext)\n",
    "                      if word not in stopwords.words() and word.isalpha()]\n",
    "    \n",
    "    candidate_sentences = {sentence: sentence.lower() for sentence in sent_tokenize(striptext)}\n",
    "    candidate_sentence_counts = {}   \n",
    "    most_frequent_words = FreqDist(lowercase_words).most_common(20)\n",
    "    \n",
    "    for long, short in candidate_sentences.items():\n",
    "        count = 0\n",
    "        for freq_word, frequency_score in most_frequent_words:\n",
    "            if freq_word in short:\n",
    "                count += frequency_score\n",
    "                candidate_sentence_counts[long] = count   \n",
    "                \n",
    "    sorted_sentences = OrderedDict(sorted(\n",
    "                        candidate_sentence_counts.items(),\n",
    "                        key = lambda x: x[1],\n",
    "                        reverse = True)[:4])\n",
    "    return sorted_sentences   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = '\\n'.join(build_naive_summary(community_data.raw()))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = '\\n'.join(build_naive_summary(le_monde_data.raw()))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can summarize George Washington's first inaugural speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_naive_summary(inaugural.raw('1789-Washington.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim: another text summarizer\n",
    "- Gensim uses a network with sentences as nodes and 'lexical similarity' as weights on the arcs between nodes<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community_root = \"data/community\"\n",
    "# le_monde_root = \"data/le_monde\"\n",
    "# community_files = \"community.*\"\n",
    "# le_monde_files = \"le_monde.*\"\n",
    "# heights_root = \"data/heights\"\n",
    "# heights_files = \"heights.*\"\n",
    "# amigos_root = \"data/amigos\"\n",
    "# amigos_files = \"amigos.*\"\n",
    "# community_data = PlaintextCorpusReader(community_root,community_files)\n",
    "# le_monde_data = PlaintextCorpusReader(le_monde_root,le_monde_files)\n",
    "# heights_data = PlaintextCorpusReader(heights_root,heights_files)\n",
    "# amigos_data = PlaintextCorpusReader(amigos_root,amigos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(community_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sentences, candidate_sentences, candidate_sentence_counts = [], {}, {}\n",
    "striptext = strip_text(community_data.raw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gensim.summarization.keywords(striptext, words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = gensim.summarization.summarize(striptext, word_count=100) \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = '\\n'.join(build_naive_summary(community_data.raw()))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Topic modeling\n",
    "---\n",
    "The goal of topic modeling is to identify the major concepts underlying a piece of text.  \n",
    "Topic modeling uses \"Unsupervised Learning\". No apriori knowledge is necessary.  \n",
    "Though it is helpful in cleaning up results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA: Latent Dirichlet Allocation Model\n",
    " - identify potential topics using pruning techniques like 'upward closure'\n",
    " - compute conditional probabilities for topic word-sets\n",
    " - identify the most likely topics, over multiple passes probabilistically picking topics in each pass\n",
    " - intuitive explanation: http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "striptext = strip_text(PlaintextCorpusReader(\"data/\", \"Nikon_coolpix_4300.txt\").raw())\n",
    "sentences = sent_tokenize(striptext)\n",
    "\n",
    "#words = word_tokenize(striptext)\n",
    "#tokenize each sentence into word tokens\n",
    "texts = [[word for word in sentence.lower().split()\n",
    "        if word not in STOPWORDS and word.isalnum()]\n",
    "        for sentence in sentences]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Create a (word, frequency) dictionary for each word in the text</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)                # (word_id, frequency) pairs\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] # (word_id, frequency) pairs by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.token2id.items();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.keys();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[73], dictionary[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA analysis\n",
    "parameters:  \n",
    " - Number of topics: The number of topics you want generated. The larger the document, the more the desirable topics\n",
    " - Passes: The LDA model makes through the document. More passes, slower analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "num_topics = 5\n",
    "passes = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus, id2word=dictionary, num_topics=num_topics, passes=passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>See results</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(lda.print_topics(num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching topics to documents\n",
    "- sort topics by probability\n",
    "- using sentences as documents here, so this is less than ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "topics = lda.get_document_topics(corpus[0], minimum_probability=0.05, per_word_topics=False)\n",
    "sorted(topics, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sense of the topics\n",
    " - draw wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_wordcloud(lda, n_topics, min_size=0, STOPWORDS=[]):\n",
    "    topics = lda.show_topic(n_topics, topn=50)\n",
    "    \n",
    "    df_ = pd.DataFrame([(word, prob) for word, prob in topics \n",
    "                        if len(word) >= min_size if word not in STOPWORDS], \n",
    "                       columns=['word', 'prob'])\n",
    "    \n",
    "    multip = 100 * df_['prob'] / df_['prob'].sum()\n",
    "    df_['multip'] =  multip.astype('int32')\n",
    "    word_list = (df_['word'] + ' ') * df_['multip']\n",
    "    text = ''.join(word_list)\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', width=3000, height=3000).generate(text)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(lda, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at Presidential addresses to see what sorts of topics emerge from there\n",
    " - Each document will be analyzed for topic</li>\n",
    " - The corpus will consist of 58 documents, one per presidential address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_WORDS = {'shall','generally','spirit','country','people','nation','nations','great','better'}\n",
    "\n",
    "# Create a word dictionary (id, word)\n",
    "texts = [[word for word in sentence.lower().split()\n",
    "        if word not in STOPWORDS and word not in REMOVE_WORDS and word.isalnum()]\n",
    "        for sentence in sentences]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Create a corpus of documents\n",
    "text_list = []\n",
    "for fileid in inaugural.fileids():\n",
    "    text = inaugural.words(fileid)\n",
    "    doc = []\n",
    "    for word in text:\n",
    "        if word in STOPWORDS or word in REMOVE_WORDS or not word.isalpha() or len(word) < 5:\n",
    "            continue\n",
    "        doc.append(word)\n",
    "    text_list.append(doc)\n",
    "    \n",
    "by_address_corpus = [dictionary.doc2bow(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(by_address_corpus, id2word=dictionary, num_topics=20, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(lda.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>We can now compare presidential addresses by topic</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(by_address_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.get_document_topics(by_address_corpus[0], minimum_probability=0, per_word_topics=False)\n",
    "sorted(topics, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(lda, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda.show_topic(12, topn=5))\n",
    "print(lda.show_topic(18, topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    " - Given a corpus of documents, when a new document arrives, find the document that is the most similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities.docsim import Similarity\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"\"\"\n",
    "Many, many years ago, I used to frequent this place for their amazing french toast. \n",
    "It's been a while since then and I've been hesitant to review a place I haven't been to in 7-8 years... \n",
    "but I passed by French Roast and, feeling nostalgic, decided to go back.\n",
    "\n",
    "It was a great decision.\n",
    "\n",
    "Their Bloody Mary is fantastic and includes bacon (which was perfectly cooked!!), olives, \n",
    "cucumber, and celery. The Irish coffee is also excellent, even without the cream which is what I ordered.\n",
    "\n",
    "Great food, great drinks, a great ambiance that is casual yet familiar like a tiny little French cafe. \n",
    "I highly recommend coming here, and will be back whenever I'm in the area next.\n",
    "\n",
    "Juan, the bartender, is great!! One of the best in any brunch spot in the city, by far.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = \"\"\"\n",
    "I went to Mexican Festival Restaurant for Cinco De Mayo because I had been there years \n",
    "prior and had such a good experience. This time wasn't so good. The food was just \n",
    "mediocre and it wasn't hot when it was brought to our table. They brought my friends food out \n",
    "10 minutes before everyone else and it took forever to get drinks. We let it slide because the place was \n",
    "packed with people and it was Cinco De Mayo. Also, the margaritas we had were slamming! Pure tequila. \n",
    "\n",
    "But then things took a turn for the worst. As I went to get something out of my purse which was on \n",
    "the back of my chair, I looked down and saw a huge water bug. I had to warn the lady next to me because \n",
    "it was so close to her chair. We called the waitress over and someone came with a broom and a dustpan and \n",
    "swept it away like it was an everyday experience. No one seemed phased.\n",
    "\n",
    "Even though our waitress was very nice, I do not think we will be returning to Mexican Festival again. \n",
    "It seems the restaurant is a shadow of its former self.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = [community_data.raw()] + [le_monde_data.raw()] + [amigos_data.raw()] + [heights_data.raw()]\n",
    "doc_list = [community_data, le_monde_data, amigos_data, heights_data]\n",
    "documents = [doc.raw() for doc in doc_list]\n",
    "assert all_text == documents\n",
    "\n",
    "texts = [[word for word in document.lower().split()\n",
    "        if word not in STOPWORDS and word.isalnum()]\n",
    "        for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lsi(texts):\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    return dictionary, corpus, models.LsiModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_similarity(doc, dictionary, corpus, lsi):\n",
    "    '''Match new doc against known docs to ge similarity'''\n",
    "    vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "    sims = index[vec_lsi]\n",
    "    return sorted(enumerate(sims), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, corpus, lsi = get_lsi(texts)\n",
    "get_doc_similarity(doc1, dictionary, corpus, lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_doc_similarity(doc2, dictionary, corpus, lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentiment analysis using logistic regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`sentiment`** data set consists of 3000 sentences which come from reviews on `imdb.com`, `amazon.com`, and `yelp.com`. Each sentence is labeled according to whether it comes from a positive review or negative review.\n",
    "\n",
    "We will use <font color=\"magenta\">logistic regression</font> to learn a classifier from this data.\n",
    "\n",
    "Before starting on this notebook, download the data from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences. The folder `sentiment_labelled_sentences` (containing the data file `full_set.txt`) should be in the same directory as the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up notebook, load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some standard includes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load in the data. Make sure the notebook is the same directory as the folder `sentiment_labelled_sentences`, and that the folder contains `full_set.txt`.\n",
    "\n",
    "The data set consists of 3000 sentences, each labeled '1' (if it came from a positive review) or '0' (if it came from a negative review). To be consistent with our notation from lecture, we will change the negative review label to '-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data set.\n",
    "with open(\"data/sentiment_labelled_sentences/full_set.txt\") as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing white space\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "# Separate the sentences from the labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text data\n",
    "\n",
    "To transform this prediction problem into one amenable to linear classification, we will first need to preprocess the text data. We will do four transformations:\n",
    "\n",
    "1. Remove punctuation and numbers.\n",
    "2. Transform all words to lower-case.\n",
    "3. Remove _stop words_.\n",
    "4. Convert the sentences into vectors, using a bag-of-words representation.\n",
    "\n",
    "We begin with first two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## full_remove takes a string x and a list of characters removal_list \n",
    "## returns x with all the characters in removal_list replaced by ' '\n",
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "digit_less = [full_remove(x, digits) for x in sentences]\n",
    "\n",
    "## Remove punctuation\n",
    "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
    "\n",
    "## Make everything lower-case\n",
    "sents_lower = [x.lower() for x in punc_less]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unwanted, short and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ' '.join([sent_words for sent_words in sents_lower])\n",
    "dictionary = set(corpus.split())\n",
    "\n",
    "# Use predefined stop words set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define our own unwanted words set\n",
    "unwanted_words = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "# Get short words\n",
    "MIN_LENGTH = 3\n",
    "short_words = set([word for word in dictionary if len(word) < MIN_LENGTH])\n",
    "\n",
    "# Define set of words to clear from text/sentences\n",
    "clear_set = stop_words | unwanted_words | short_words\n",
    "\n",
    "# Clear text from unwanted words\n",
    "sents_split = [x.split() for x in sents_lower]\n",
    "sents_processed = [' '.join(list(filter(lambda word: word not in clear_set, sent_words))) for sent_words in sents_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the sentences look like so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_processed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "In order to use linear classifiers on our data set, we need to transform our textual data into numeric data. \n",
    "The classical way to do this is known as the _bag of words_ representation. \n",
    "\n",
    "_bag of words_ representation: \n",
    " - each word is thought of as corresponding to a number in `{1, 2, ..., V}` where `V` is the size of our vocabulary. \n",
    " - each sentence is represented as a V-dimensional vector $x$, where $x_i$ is the number of times that word $i$ occurs in the sentence.\n",
    "\n",
    "We use of the `CountVectorizer` class in `scikit-learn` for the transformation.\n",
    "\n",
    "We will cap the number of features at 4500, meaning;\n",
    " - a word will make it into our vocabulary only if it is one of the 4500 most common words in the corpus\n",
    " - this can weed out spelling mistakes and words which occur too infrequently to be useful\n",
    " - we will also append a '1' to the end of each vector to allow our linear classifier to learn a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Transform to bag of words representation.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 4500)\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "\n",
    "# Append '1' to the end of each vector.\n",
    "p_mat = data_features.toarray()\n",
    "data_mat = np.ones((p_mat.shape[0], p_mat.shape[1]+1))\n",
    "data_mat[:,:-1] = p_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / test split\n",
    "\n",
    "Finally, we split the data into a training set of 2500 sentences and a test set of 500 sentences (of which 250 are positive and 250 negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into testing and training sets\n",
    "\n",
    "# Random stratified selection of the indices \n",
    "np.random.seed(0)\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), \n",
    "                      np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "all_inds = range(len(labels))\n",
    "train_inds = list(set(all_inds) - set(test_inds))\n",
    "\n",
    "train_data = data_mat[train_inds,]\n",
    "train_labels = y[train_inds]\n",
    "\n",
    "test_data = data_mat[test_inds,]\n",
    "test_labels = y[test_inds]\n",
    "\n",
    "print(\"train data: \", train_data.shape)\n",
    "print(\"test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting a logistic regression model to the training data\n",
    "\n",
    "We use logistic regression classifier, using stochastic gradient descent (SGD), from `scikit-learn`.  \n",
    "Randomness in the SGD procedure yields slightly different solutions (and thus different error values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "## Fit logistic classifier on training data\n",
    "clf = SGDClassifier(loss='log', penalty='l2', max_iter=200)\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w = clf.coef_[0,:]\n",
    "b = clf.intercept_\n",
    "\n",
    "## Get predictions on training and test data\n",
    "preds_train = clf.predict(train_data)\n",
    "preds_test = clf.predict(test_data)\n",
    "\n",
    "## Compute errors\n",
    "errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n",
    "\n",
    "print('Training error: {}'.format(float(errs_train)/len(train_labels)))\n",
    "print('Test error: {}'.format(float(errs_test)/len(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing the margin\n",
    "\n",
    "The logistic regression model produces not just classifications but also conditional probability estimates. \n",
    "We will say that `x` has **margin** `gamma` if (according to the logistic regression model)  \n",
    "`Pr(y=1|x) > (1/2)+gamma`  \n",
    "or  \n",
    "`Pr(y=1|x) < (1/2)-gamma`  \n",
    "\n",
    "The following function **margin_counts** computes how many points in the test-set have margin of at least `gamma`.\n",
    "input:\n",
    "1. the classifier (`clf`, computed earlier)\n",
    "2. the test set (`test_data`)\n",
    "3. a value of `gamma`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return number of test points for which Pr(y=1) lies in [0, 0.5 - gamma) or (0.5 + gamma, 1]\n",
    "def margin_dist(clf, test_data, gamma):\n",
    "    # Compute probability on each test point\n",
    "    proba = clf.predict_proba(test_data)[:, 1]\n",
    "    # Find data points for which prediction is at least gamma away from 0.5\n",
    "    margin_inds = np.where((proba > (0.5+gamma)) | (proba < (0.5-gamma)))[0]\n",
    "    \n",
    "    return float(len(margin_inds))/len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the test set's distribution of margin values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = np.arange(0, 0.5, 0.01)\n",
    "f = np.vectorize(lambda g: margin_dist(clf, test_data, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gammas, f(gammas), linewidth=2, color='green')\n",
    "plt.xlabel('Margin', fontsize=14)\n",
    "plt.ylabel('Fraction of points above margin', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we investigate a natural question: <font color=\"magenta\">Are points `x` with larger margin more likely to be classified correctly?</font>\n",
    "\n",
    "To address this, we define a function **margin_errors** that computes the fraction of points with margin at least `gamma` that are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return error of predictions that lie in intervals [0, 0.5 - gamma) and (0.5 + gamma, 1]\n",
    "def margin_errors(clf, test_data, test_labels, gamma):\n",
    "    # Compute probability on each test point\n",
    "    preds = clf.predict_proba(test_data)[:, 1]\n",
    "    \n",
    "    # Find data points for which prediction is at least gamma away from 0.5\n",
    "    margin_inds = np.where((preds > (0.5+gamma)) | (preds < (0.5-gamma)))[0]\n",
    "    \n",
    "    # Compute error on those data points.\n",
    "    num_errors = np.sum((preds[margin_inds] > 0.5) != (test_labels[margin_inds] > 0.0))\n",
    "    return float(num_errors)/len(margin_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the relationship between margin and error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create grid of gamma values\n",
    "gammas = np.arange(0, 0.5, 0.01)\n",
    "\n",
    "## Compute margin_errors on test data for each value of g\n",
    "f = np.vectorize(lambda g: margin_errors(clf, test_data, test_labels, g))\n",
    "\n",
    "## Plot the result\n",
    "plt.plot(gammas, f(gammas), linewidth=2)\n",
    "plt.ylabel('Error rate', fontsize=14)\n",
    "plt.xlabel('Margin', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Words with large influence\n",
    "\n",
    "Finally, we attempt to partially **interpret** the logistic regression model.\n",
    "\n",
    "Which words are most important in deciding whether a sentence is positive? As a first approximation to this, we simply take the words whose coefficients in `w` have the largest positive values.\n",
    "\n",
    "Likewise, we look at the words whose coefficients in `w` have the most negative values, and we think of these as influential in negative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert bow into a Series, automatically sorted:\n",
    "bow_series = pd.Series({v:k for k, v in vectorizer.vocabulary_.items()})\n",
    "\n",
    "## Get indices of sorting w\n",
    "inds = np.argsort(w)\n",
    "\n",
    "# First/last number of words\n",
    "N = 10\n",
    "\n",
    "## Words with large negative values\n",
    "neg_inds = inds[:N]\n",
    "print(\"Highly negative words:\\n{}\".format(bow_series[inds[:N]].values))\n",
    "print(\"Highly positive words:\\n{}\".format(bow_series[inds[-N:]].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Unresolved, undecided\n",
    "\n",
    "Suppose you are building a classifier, and can tolerate an error rate of at most some value `e`. Unfortunately, every classifier you try has a higher error than this. \n",
    "\n",
    "Therefore, you decide that the classifier is allowed to occasionally **abstain**: that is, to say *\"don't know\"*. When it actually makes a prediction, it must have error rate at most `e`. And subject to this constraint, it should abstain as infrequently as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter data\n",
    "\n",
    "<div class=note><b>Copyright and Licensing:</b>\n",
    "\n",
    "\n",
    "You are free to use or adapt this notebook for any purpose you'd like. However, please respect the [Simplified BSD License](https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/blob/master/LICENSE.txt) that governs its use.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API Access\n",
    "\n",
    "Twitter implements OAuth 1.0A as its standard authentication mechanism, and in order to use it to make requests to Twitter's API, you'll need to go to https://dev.twitter.com/apps and create a sample application.\n",
    "\n",
    "Choose any name for your application, write a description and use `http://google.com` for the website.\n",
    "\n",
    "Under **Key and Access Tokens**, there are four primary identifiers you'll need to note for an OAuth 1.0A workflow: \n",
    "* consumer key, \n",
    "* consumer secret, \n",
    "* access token, and \n",
    "* access token secret (Click on Create Access Token to create those).\n",
    "\n",
    "Note that you will need an ordinary Twitter account in order to login, create an app, and get these credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time you execute the notebook, add all credentials so that you can save them in the `pkl` file, then you can remove the secret keys from the notebook because they will just be loaded from the `pkl` file.\n",
    "\n",
    "The `pkl` file contains sensitive information that can be used to take control of your twitter acccount, **do not share it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../_data/standard_import.txt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../_credentials/twitter_credentials.txt'\n",
    "pickle_file = '../_credentials/twitter_credentials.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(pickle_file):\n",
    "    Twitter={}\n",
    "    Twitter['Consumer Key'] = 'p...6XH'\n",
    "    Twitter['Consumer Secret'] = 'Y...4yR'\n",
    "    Twitter['Access Token'] = '2...7eQ'\n",
    "    Twitter['Access Token Secret'] = '1...w53'\n",
    "    with open(pickle_file,'wb') as f:\n",
    "        pickle.dump(Twitter, f)\n",
    "else:\n",
    "    Twitter=pickle.load(open(pickle_file,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the `twitter` package to interface with the Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorizing an application to access Twitter account data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "auth = twitter.oauth.OAuth(Twitter['Access Token'],\n",
    "                           Twitter['Access Token Secret'],\n",
    "                           Twitter['Consumer Key'],\n",
    "                           Twitter['Consumer Secret'])\n",
    "\n",
    "twitter_api = twitter.Twitter(auth=auth)\n",
    "\n",
    "# Nothing to see by displaying twitter_api except that it's now a\n",
    "# defined variable\n",
    "\n",
    "print(twitter_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter identifies locations using the __Yahoo! Where On Earth ID__.\n",
    "\n",
    "The Yahoo! Where On Earth ID for the entire world is 1.  \n",
    "[Find your WOE ID](http://www.woeidlookup.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD_WOE_ID = 1\n",
    "NL_WOE_ID = 23424909\n",
    "US_WOE_ID = 23424977\n",
    "LOCAL_WOE_ID = 727232 # Amsterdam, NL\n",
    "\n",
    "# Too local to get tweets\n",
    "BB_WOE_ID = 727407    # Baambrugge - De Ronde Venen, NL\n",
    "ABC_WOE_ID = 727050   # Abcoude - De Ronde Venen, NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix ID with the underscore for query string parameterization.\n",
    "# Without the underscore, the twitter package appends the ID value\n",
    "# to the URL itself as a special case keyword argument.\n",
    "\n",
    "world_trends = twitter_api.trends.place(_id=WORLD_WOE_ID)\n",
    "nl_trends = twitter_api.trends.place(_id=NL_WOE_ID)\n",
    "local_trends = twitter_api.trends.place(_id=LOCAL_WOE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traversing through the nested dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_trends\n",
    "dict_local_trends = local_trends[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict_local_trends.keys())\n",
    "dict_local_trends['trends']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x['name'] for x in local_trends[0]['trends']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display dictionary as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends = pd.DataFrame(dict_local_trends['trends'])\n",
    "df_trends.sort_values('tweet_volume', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print((json.dumps(local_trends[:2], indent=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the intersection of two sets of trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_set = {}\n",
    "trends_set['world'] = set([trend['name'] for trend in world_trends[0]['trends']])\n",
    "trends_set['nl'] = set([trend['name'] for trend in nl_trends[0]['trends']]) \n",
    "trends_set['amsterdam'] = set([trend['name'] for trend in local_trends[0]['trends']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in trends_set.keys():\n",
    "    print('\\n------------ {} trends-----------\\n'.format(loc))\n",
    "    print((', '.join(trends_set[loc])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*10 + '> World & NL\\n')\n",
    "print((trends_set['world'].intersection(trends_set['nl'])))\n",
    "print()\n",
    "print('='*10 + '> NL & Amsterdam\\n')\n",
    "print((trends_set['nl'].intersection(trends_set['amsterdam'])))\n",
    "print()\n",
    "print('='*10 + '> World & NL & Amsterdam\\n')\n",
    "print((trends_set['amsterdam'].intersection(trends_set['nl'])).intersection(trends_set['world']))\n",
    "print()\n",
    "print('='*10 + '> World (NOT NL) \\n')\n",
    "print((trends_set['nl'] ^ trends_set['world']).intersection(trends_set['world']))\n",
    "print()\n",
    "print('='*10 + '> NL (NOT World) \\n')\n",
    "print((trends_set['nl'] ^ trends_set['world']).intersection(trends_set['nl']))\n",
    "print()\n",
    "print('='*10 + '> Amsterdam (NOT NL) \\n')\n",
    "print((trends_set['amsterdam'] ^ trends_set['nl']).intersection(trends_set['amsterdam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting search results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the variable `q` to a trending topic, \n",
    "or anything else for that matter. The example query below\n",
    "was a trending topic when this content was being developed\n",
    "and is used throughout the remainder of this chapter\n",
    "\n",
    "[api docs](https://dev.twitter.com/docs/api/1.1/get/search/tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_local_trends = (trends_set['amsterdam'] ^ trends_set['nl']).intersection(trends_set['amsterdam'])\n",
    "\n",
    "q = list(only_local_trends)[0] #'#MTVAwards' \n",
    "number = 100\n",
    "\n",
    "search_results = twitter_api.search.tweets(q=q, count=number)\n",
    "statuses = search_results['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(statuses)\n",
    "statuses[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s['text'] for s in search_results['statuses']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete duplicate tweets\n",
    "Twitter often returns duplicate results, we can filter them out checking for duplicate texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = []\n",
    "filtered_statuses = []\n",
    "for s in statuses:\n",
    "    if not s[\"text\"] in all_text:\n",
    "        filtered_statuses.append(s)\n",
    "        all_text.append(s[\"text\"])\n",
    "statuses = filtered_statuses     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show one sample search result by slicing the list...\n",
    "print(json.dumps(statuses[0], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result of the list comprehension is a list with only one element that\n",
    "# can be accessed by its index and set to the variable t\n",
    "t = statuses[0]\n",
    "\n",
    "#[status for status in statuses \n",
    "#          if status['id']==316948241264549888][0]]\n",
    "\n",
    "# Explore the variable t to get familiarized with the data structure...\n",
    "statuses[0]['retweet_count']\n",
    "statuses[0]['retweeted']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting text, screen names, and hashtags from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_texts = [status['text'] for status in statuses]\n",
    "\n",
    "screen_names = [user_mention['screen_name'] for status in statuses\n",
    "                                            for user_mention in status['entities']['user_mentions']]\n",
    "\n",
    "hashtags = [hashtag['text'].lower() for status in statuses\n",
    "                            for hashtag in status['entities']['hashtags']]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [w.lower() for t in status_texts \n",
    "           for w in t.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the first 5 items for each...\n",
    "print('status text: ', json.dumps(status_texts[0:5], indent=1))\n",
    "print('screen names: ', json.dumps(screen_names[0:5], indent=1)) \n",
    "print('hashtags: ', json.dumps(hashtags[0:5], indent=1))\n",
    "print('words: ', json.dumps(words[0:5], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic frequency distribution from the words in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for item in [words, screen_names, hashtags]:\n",
    "    c = Counter(item)\n",
    "    print('-'*80)\n",
    "    print(c.most_common()[:10]) # top 10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Counter(words).most_common(30), columns=['word', 'count']).set_index('word').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Counter(screen_names).most_common(30), columns=['mentions', 'count']).set_index('mentions').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Counter(hashtags).most_common(30), columns=['hashtags', 'count']).set_index('hashtags').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets = [\n",
    "            # Store out a tuple of these three values ...\n",
    "            (status['retweet_count'], \n",
    "             status['retweeted_status']['user']['screen_name'],\n",
    "             status['text'].replace(\"\\n\",\"\\\\\")) \n",
    "            \n",
    "            # ... for each status ...\n",
    "            for status in statuses \n",
    "            \n",
    "            # ... so long as the status meets this condition.\n",
    "                if 'retweeted_status' in status\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retweets = pd.DataFrame(retweets, columns=['retweets', 'screen_name', 'text']).sort_values('retweets', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retweets.text[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

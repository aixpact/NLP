{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You are currently looking at **version 1.0** of this notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topic modeling\n",
    "The goal of topic modeling is to identify the major concepts underlying a piece of text.  \n",
    "Topic modeling uses \"Unsupervised Learning\". No apriori knowledge is necessary.  \n",
    "Though it is helpful in cleaning up results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup notebook\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the generic libraries used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from collections import OrderedDict, Counter\n",
    "import pprint\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set defaults and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas defaults\n",
    "pd.set_option('max_rows', 10)                                # Show max 10 rows: head(5) ... tail(5)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)  # Set precision of DataFrames/Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check current working directory and file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download() # download datasets = corpi\n",
    "from nltk import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords, inaugural, PlaintextCorpusReader\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA: Latent Dirichlet Allocation Model\n",
    " - identify potential topics using pruning techniques like 'upward closure'\n",
    " - compute conditional probabilities for topic word-sets\n",
    " - identify the most likely topics, over multiple passes probabilistically picking topics in each pass\n",
    " - intuitive explanation: http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "striptext = strip_text(PlaintextCorpusReader(\"data/\", \"Nikon_coolpix_4300.txt\").raw())\n",
    "sentences = sent_tokenize(striptext)\n",
    "\n",
    "#words = word_tokenize(striptext)\n",
    "#tokenize each sentence into word tokens\n",
    "texts = [[word for word in sentence.lower().split()\n",
    "        if word not in STOPWORDS and word.isalnum()]\n",
    "        for sentence in sentences]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Create a (word, frequency) dictionary for each word in the text</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)                # (word_id, frequency) pairs\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] # (word_id, frequency) pairs by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.token2id.items();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.keys();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[73], dictionary[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA analysis\n",
    "parameters:  \n",
    " - Number of topics: The number of topics you want generated. The larger the document, the more the desirable topics\n",
    " - Passes: The LDA model makes through the document. More passes, slower analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "num_topics = 5\n",
    "passes = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus, id2word=dictionary, num_topics=num_topics, passes=passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>See results</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(lda.print_topics(num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching topics to documents\n",
    "- sort topics by probability\n",
    "- using sentences as documents here, so this is less than ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "topics = lda.get_document_topics(corpus[0], minimum_probability=0.05, per_word_topics=False)\n",
    "sorted(topics, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sense of the topics\n",
    " - draw wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_wordcloud(lda, n_topics, min_size=0, STOPWORDS=[]):\n",
    "    topics = lda.show_topic(n_topics, topn=50)\n",
    "    \n",
    "    df_ = pd.DataFrame([(word, prob) for word, prob in topics \n",
    "                        if len(word) >= min_size if word not in STOPWORDS], \n",
    "                       columns=['word', 'prob'])\n",
    "    \n",
    "    multip = 100 * df_['prob'] / df_['prob'].sum()\n",
    "    df_['multip'] =  multip.astype('int32')\n",
    "    word_list = (df_['word'] + ' ') * df_['multip']\n",
    "    text = ''.join(word_list)\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', width=3000, height=3000).generate(text)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(lda, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at Presidential addresses to see what sorts of topics emerge from there\n",
    " - Each document will be analyzed for topic</li>\n",
    " - The corpus will consist of 58 documents, one per presidential address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_WORDS = {'shall','generally','spirit','country','people','nation','nations','great','better'}\n",
    "\n",
    "# Create a word dictionary (id, word)\n",
    "texts = [[word for word in sentence.lower().split()\n",
    "        if word not in STOPWORDS and word not in REMOVE_WORDS and word.isalnum()]\n",
    "        for sentence in sentences]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Create a corpus of documents\n",
    "text_list = []\n",
    "for fileid in inaugural.fileids():\n",
    "    text = inaugural.words(fileid)\n",
    "    doc = []\n",
    "    for word in text:\n",
    "        if word in STOPWORDS or word in REMOVE_WORDS or not word.isalpha() or len(word) < 5:\n",
    "            continue\n",
    "        doc.append(word)\n",
    "    text_list.append(doc)\n",
    "    \n",
    "by_address_corpus = [dictionary.doc2bow(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(by_address_corpus, id2word=dictionary, num_topics=20, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(lda.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>We can now compare presidential addresses by topic</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(by_address_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.get_document_topics(by_address_corpus[0], minimum_probability=0, per_word_topics=False)\n",
    "sorted(topics, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(lda, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda.show_topic(12, topn=5))\n",
    "print(lda.show_topic(18, topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    " - Given a corpus of documents, when a new document arrives, find the document that is the most similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities.docsim import Similarity\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"\"\"\n",
    "Many, many years ago, I used to frequent this place for their amazing french toast. \n",
    "It's been a while since then and I've been hesitant to review a place I haven't been to in 7-8 years... \n",
    "but I passed by French Roast and, feeling nostalgic, decided to go back.\n",
    "\n",
    "It was a great decision.\n",
    "\n",
    "Their Bloody Mary is fantastic and includes bacon (which was perfectly cooked!!), olives, \n",
    "cucumber, and celery. The Irish coffee is also excellent, even without the cream which is what I ordered.\n",
    "\n",
    "Great food, great drinks, a great ambiance that is casual yet familiar like a tiny little French cafe. \n",
    "I highly recommend coming here, and will be back whenever I'm in the area next.\n",
    "\n",
    "Juan, the bartender, is great!! One of the best in any brunch spot in the city, by far.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = \"\"\"\n",
    "I went to Mexican Festival Restaurant for Cinco De Mayo because I had been there years \n",
    "prior and had such a good experience. This time wasn't so good. The food was just \n",
    "mediocre and it wasn't hot when it was brought to our table. They brought my friends food out \n",
    "10 minutes before everyone else and it took forever to get drinks. We let it slide because the place was \n",
    "packed with people and it was Cinco De Mayo. Also, the margaritas we had were slamming! Pure tequila. \n",
    "\n",
    "But then things took a turn for the worst. As I went to get something out of my purse which was on \n",
    "the back of my chair, I looked down and saw a huge water bug. I had to warn the lady next to me because \n",
    "it was so close to her chair. We called the waitress over and someone came with a broom and a dustpan and \n",
    "swept it away like it was an everyday experience. No one seemed phased.\n",
    "\n",
    "Even though our waitress was very nice, I do not think we will be returning to Mexican Festival again. \n",
    "It seems the restaurant is a shadow of its former self.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = [community_data.raw()] + [le_monde_data.raw()] + [amigos_data.raw()] + [heights_data.raw()]\n",
    "doc_list = [community_data, le_monde_data, amigos_data, heights_data]\n",
    "documents = [doc.raw() for doc in doc_list]\n",
    "assert all_text == documents\n",
    "\n",
    "texts = [[word for word in document.lower().split()\n",
    "        if word not in STOPWORDS and word.isalnum()]\n",
    "        for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lsi(texts):\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    return dictionary, corpus, models.LsiModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_similarity(doc, dictionary, corpus, lsi):\n",
    "    '''Match new doc against known docs to ge similarity'''\n",
    "    vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "    sims = index[vec_lsi]\n",
    "    return sorted(enumerate(sims), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, corpus, lsi = get_lsi(texts)\n",
    "get_doc_similarity(doc1, dictionary, corpus, lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_doc_similarity(doc2, dictionary, corpus, lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

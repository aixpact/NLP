{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweepy\n",
    "\n",
    "[Tweepy](http://tweepy.readthedocs.io/en/v3.5.0/)  \n",
    "[marcobonzanini.com](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../_data/standard_import.txt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import twitter\n",
    "import json\n",
    "\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = '../_credentials/twitter_credentials.pkl'\n",
    "Twitter=pickle.load(open(pickle_file, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authorizing an application to access Twitter account data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = 'YOUR-CONSUMER-KEY'\n",
    "consumer_secret = 'YOUR-CONSUMER-SECRET'\n",
    "access_token = 'YOUR-ACCESS-TOKEN'\n",
    "access_secret = 'YOUR-ACCESS-SECRET'\n",
    " \n",
    "auth = OAuthHandler(Twitter['Consumer Key'], Twitter['Consumer Secret'])\n",
    "auth.set_access_token(Twitter['Access Token'], Twitter['Access Token Secret'])\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.wait_on_rate_limit = True\n",
    "api.wait_on_rate_limit_notify = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get current address and lat, long with HERE api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import platform\n",
    "import re\n",
    "import ssl\n",
    "import subprocess\n",
    "import requests\n",
    "import urllib.request\n",
    "from urllib.request import urlretrieve\n",
    "import plistlib\n",
    "import tweepy\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today = datetime.date.today()\n",
    "yesterday = today - datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "\n",
    "pickle_file='../_credentials/here_credentials.pkl'\n",
    "Here=pickle.load(open(pickle_file,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_latlon_address(address):\n",
    "    \"\"\"Lookup address details as JSON using HERE api\n",
    "    Decode UTF-8 bytes to Unicode and convert single quotes to double quotes to make it valid JSON.\n",
    "    Load the JSON to a Python list & dump it back out as formatted JSON.\"\"\"\n",
    "    import os, pickle, requests, json\n",
    "    \n",
    "    Here=pickle.load(open('../_credentials/here_credentials.pkl','rb'))\n",
    "    api_url = 'https://geocoder.cit.api.here.com/6.2/geocode.json'\n",
    "    http = '{}?app_id={}&app_code={}&searchtext={}'.format(api_url, Here['app_id'], Here['app_code'], address)\n",
    "    content = requests.get(http).content\n",
    "    my_json = content.decode('utf8').replace(\"'\", '\"')\n",
    "    data = json.loads(my_json)\n",
    "    latlon_dict = data['Response']['View'][0]['Result'][0]['Location']['NavigationPosition'][0]\n",
    "    return latlon_dict['Latitude'], latlon_dict['Longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_latlon_address('9, dorpsstraat, baambrugge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def payload():\n",
    "    \"\"\"Returns payload for POST request HERE api.\"\"\"\n",
    "    import re, ast\n",
    "    \n",
    "    hotspots = !airport scan\n",
    "    pos_bssid = re.search(r'\\b(BSSID)\\b', hotspots[0]).start()\n",
    "    bssid = {b[pos_bssid:pos_bssid+17]:b[pos_bssid+18:pos_bssid+21] for b in hotspots[1:]}\n",
    "    payload = {} \n",
    "    payload['wlan'] = [ast.literal_eval('{}\"mac\": \"{}\", \"powrx\": {}{}'.format('{', b, int(r), '}')) for b, r in bssid.items()]\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_latlon():\n",
    "    \"\"\"Lookup address details as JSON using HERE api\n",
    "    Decode UTF-8 bytes to Unicode and convert single quotes to double quotes to make it valid JSON.\n",
    "    Load the JSON to a Python list & dump it back out as formatted JSON.\"\"\"\n",
    "    import requests, json\n",
    "    \n",
    "    api_url = 'https://pos.cit.api.here.com/positioning/v1/locate'\n",
    "    http = '{}?app_id={}&app_code={}'.format(api_url, Here['app_id'], Here['app_code'])\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    \n",
    "    r = requests.post(http, data=json.dumps(payload()), headers=headers)\n",
    "    print(r.status_code, r.reason)\n",
    "    my_json = r.content.decode('utf8').replace(\"'\", '\"')\n",
    "    \n",
    "    return json.loads(my_json)['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_latlon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_address():\n",
    "    \"\"\"Reverse get address from latlon.\"\"\"\n",
    "    \n",
    "    import ssl, urllib.request, json\n",
    "    \n",
    "    geo_latlonrad = geo_latlon()\n",
    "    lat, lon, radius = geo_latlonrad['lat'], geo_latlonrad['lng'], geo_latlonrad['accuracy']\n",
    "    \n",
    "    context = ssl._create_unverified_context()\n",
    "    rev_geocoder_url = 'https://reverse.geocoder.api.here.com/6.2/reversegeocode.json?prox='\n",
    "    \n",
    "    results = json.load(urllib.request.urlopen(\n",
    "        '{}{},{},1000&mode=retrieveAddresses&maxResults=3&gen=8&app_id={}&app_code={}'.format(\n",
    "            rev_geocoder_url, lat, lon, Here['app_id'], Here['app_code']), context=context)\n",
    "                       )['Response']['View'][0]['Result']\n",
    "    \n",
    "    for result in results:\n",
    "        if result['MatchQuality'].get('Street'):\n",
    "            address = result['Location']['Address']['Label']\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return address\n",
    "geo_address()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon = geo_latlon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local topics by lat lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_search(keyword, radius, lang='nl', count=100):\n",
    "    \"\"\"Get tweets by topic within radius from current location\n",
    "    http://docs.tweepy.org/en/v3.5.0/api.html#status-methods\"\"\"\n",
    "    import datetime\n",
    "    tweet_list = []\n",
    "    geocode = '{},{},{}km'.format(latlon['lat'], latlon['lng'], radius)\n",
    "    tweets = tweepy.Cursor(api.search, q=keyword, lang=lang, geocode=geocode, since=str(yesterday)).items(count)\n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            tweet_list.append([tweet.geo['coordinates'], tweet.text])\n",
    "        except:\n",
    "            continue\n",
    "    # dir(tweet): tweet.created_at, tweet.user.id , tweet.user.followers_count\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_lon(address):\n",
    "    \"\"\"Return lat, lon, accuracy based on address(country, place, street)\"\"\"\n",
    "    from geopy.geocoders import Nominatim\n",
    "    geolocator = Nominatim()\n",
    "    location = geolocator.geocode(address)\n",
    "    return {'lat': location.latitude, 'lng': location.longitude,'accuracy': 0.}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'japan'\n",
    "radius = 20\n",
    "lang = 'nl'\n",
    "# loc_search = local_search(keyword, radius, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocode = '{},{},{}km'.format(latlon['lat'], latlon['lng'], radius)\n",
    "tweet_latlon = []\n",
    "for t in tweepy.Cursor(api.search, q=keyword, lang=lang, geocode=geocode, since=str(yesterday)).items(200):\n",
    "    try: \n",
    "        print('{} ({}{}): \\n{}'.format(t.user.name, t.user.location, get_lat_lon(t.user.location), t.text))\n",
    "        tweet_latlon.append(get_lat_lon(t.user.location))\n",
    "#         print(api.get_status(t.id).geo) # dir(t)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Distance between 2 geographic locations by lat-lon coordinates.\"\"\"\n",
    "    miles_constant = 3959\n",
    "    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1 \n",
    "    dlon = lon2 - lon1 \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    mi = miles_constant * c\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_latlon[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.DataFrame(tweet_latlon, columns=['lat', 'lng', 'accuracy'])\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['distance'] = haversine(latlon['lat'], latlon['lng'], df_tweets['lat'].values, df_tweets['lng'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create bounding box\n",
    "\n",
    "Convert radius in kilometers to degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = radius * 10\n",
    "zoom_level = int(20/np.log(area)//1 + 4)\n",
    "\n",
    "lat_deg = area/110.574\n",
    "lon_deg = area/(111.320 * np.cos(latlon['lat']))\n",
    "bbox = [latlon['lng']-lon_deg, latlon['lng']+lon_deg, latlon['lat']-lat_deg, latlon['lat']+lat_deg]\n",
    "bbox, zoom_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "from cartopy.io.img_tiles import OSM # maps\n",
    "\n",
    "osm_tiles = OSM()\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "# Use the tile's projection for the underlying map.\n",
    "ax = plt.axes(projection=osm_tiles.crs)\n",
    "\n",
    "# Specify a region of interest, in this case, Cardiff.\n",
    "ax.set_extent(bbox, ccrs.PlateCarree())\n",
    "\n",
    "# Add the tiles at zoom level\n",
    "# zoom level determines the detail level of the bbox AND \n",
    "# the combination of radius and zoom level determines the loading time(9 ~ too coarse, 11 ~ several minutes)\n",
    "# zoomlevel 10: country level - area 20\n",
    "# zoomlevel 9: country level - area 100\n",
    "# zoomlevel 8: country level - area 200\n",
    "# zoomlevel 7: country level - area 300\n",
    "# zoom_level = int(20/np.log(x)//1 + 4)\n",
    "ax.add_image(osm_tiles, zoom_level)\n",
    "\n",
    "# Add tweets to plot\n",
    "plt.plot(df_tweets.loc[df_tweets['distance'] <= radius, 'lng'], \n",
    "         df_tweets.loc[df_tweets['distance'] <= radius, 'lat'], \n",
    "         transform=ccrs.PlateCarree(),\n",
    "         marker='o', \n",
    "         color='blue', \n",
    "         markersize=12, \n",
    "         alpha=.4,\n",
    "         linestyle='')\n",
    "\n",
    "plt.plot(df_tweets.loc[df_tweets['distance'] > radius, 'lng'], \n",
    "         df_tweets.loc[df_tweets['distance'] > radius, 'lat'], \n",
    "         transform=ccrs.PlateCarree(),\n",
    "         marker='o', \n",
    "         color='red', \n",
    "         markersize=12, \n",
    "         alpha=.4,\n",
    "         linestyle='')\n",
    "\n",
    "# ax.coastlines('10m')\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read our own timeline (i.e. our Twitter homepage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in tweepy.Cursor(api.home_timeline).items(10):\n",
    "    # Process a single status\n",
    "    print(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function process_or_store() is a place-holder for your custom implementation. In the simplest form, you could just print out the JSON, one tweet per line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_or_store(tweet):\n",
    "    return json.dumps(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in tweepy.Cursor(api.home_timeline).items(10):\n",
    "    # Process a single status\n",
    "    status._json['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of all your followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for friend in tweepy.Cursor(api.friends).items(10):\n",
    "    friend._json['screen_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of all your tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweepy.Cursor(api.user_timeline).items(10):\n",
    "    tweet._json['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way we can easily collect tweets (and more) and store them in the original JSON format,  \n",
    "fairly easy to convert into different data models depending on our storage (many NoSQL technologies provide some bulk import feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('python.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "    \n",
    "# twitter_stream = Stream(auth, MyListener())\n",
    "# twitter_stream.filter(track=['#python'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head python.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "with open('python.json', 'r') as f:\n",
    "    line = f.readline() # read only the first tweet/line\n",
    "    tweet = json.loads(line) # load it as Python dict\n",
    "    print(json.dumps(tweet, indent=4)) # pretty-print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(preprocess(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('python.json', 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tokens = preprocess(tweet['text'])\n",
    "        print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "fname = 'python.json'\n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_all)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "# We can now substitute the variable terms_all in the first example with something like:\n",
    "\n",
    "terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More term filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count terms only once, equivalent to Document Frequency\n",
    "terms_single = set(terms_all)\n",
    "# Count hashtags only\n",
    "terms_hash = [term for term in preprocess(tweet['text']) \n",
    "              if term.startswith('#')]\n",
    "# Count terms only (no hashtags, no mentions)\n",
    "terms_only = [term for term in preprocess(tweet['text']) \n",
    "              if term not in stop and\n",
    "              not term.startswith(('#', '@'))] \n",
    "              # mind the ((double brackets))\n",
    "              # startswith() takes a tuple (not a list) if \n",
    "              # we pass a list of inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams \n",
    " \n",
    "terms_bigram = bigrams(terms_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Yahoo! Where On World ID dictionary\n",
    "\n",
    "The Yahoo! Where On Earth ID for the entire world is 1.  \n",
    "[Find your WOE ID](http://woeid.rosselliot.co.nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_woeid(locations):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    woeids = {}\n",
    "    if type(locations) != type([]): \n",
    "        locations = [locations]\n",
    "    for loc in locations:\n",
    "        try:\n",
    "            response = requests.get('http://woeid.rosselliot.co.nz/lookup/' + loc)\n",
    "            results_page = BeautifulSoup(response.content, 'lxml')\n",
    "            woeids[loc] = int(results_page.find_all('td', {\"class\": \"woeid\"})[0].text)\n",
    "        except:\n",
    "            continue\n",
    "    return woeids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATIONS = ['World', 'United States', 'Netherlands', 'Europe', 'Amsterdam', 'Abcoude']\n",
    "WOEID = get_woeid(LOCATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in LOCATIONS:\n",
    "    print(loc, WOEID[loc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends by tweet volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def woe_trends_(location):\n",
    "    \"\"\"Returns localised trends\"\"\"\n",
    "    WOEID = get_woeid(location)\n",
    "    local_trends = twitter_api.trends.place(_id=WOEID[location])[0]['trends']\n",
    "    return (pd.DataFrame([(trend['name'],trend['tweet_volume']) \n",
    "                         for trend in local_trends], \n",
    "                         columns=['trend', 'volume'])\n",
    "                         .sort_values('volume', ascending=False)\n",
    "                         .set_index('trend'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_trends_('Amsterdam').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locality of trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trending_location(location):\n",
    "    \"\"\"Returns localised trends\"\"\"\n",
    "    WOE_ID = get_woeid(location)\n",
    "    local_trends = twitter_api.trends.place(_id=WOE_ID[location])[0]['trends']\n",
    "    return set([trend['name'] for trend in local_trends])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trending_shared(locations):\n",
    "    \"\"\"Returns trends shared between WOE_ID_1 & WOE_ID_2\"\"\"\n",
    "    WOE_ID = get_woeid(locations)\n",
    "    local_trends_1 = twitter_api.trends.place(_id=WOE_ID[locations[0]])[0]['trends']\n",
    "    local_trends_2 = twitter_api.trends.place(_id=WOE_ID[locations[1]])[0]['trends']\n",
    "    set_1 = set([trend['name'] for trend in local_trends_1])\n",
    "    set_2 = set([trend['name'] for trend in local_trends_2])\n",
    "    return list(set_1 & set_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trending_excl(locations):\n",
    "    \"\"\"Returns trends only in WOE_ID_1\"\"\"\n",
    "    WOE_ID = get_woeid(locations)\n",
    "    local_trends_1 = twitter_api.trends.place(_id=WOE_ID[locations[0]])[0]['trends']\n",
    "    local_trends_2 = twitter_api.trends.place(_id=WOE_ID[locations[1]])[0]['trends']\n",
    "    set_1 = set([trend['name'] for trend in local_trends_1])\n",
    "    set_2 = set([trend['name'] for trend in local_trends_2])\n",
    "    return list((set_1 ^ set_2) & set_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_set = {}\n",
    "trends_set['world'] = trending_location('World')\n",
    "trends_set['nl'] = trending_location('Netherlands')\n",
    "trends_set['amsterdam'] = trending_location('Amsterdam')\n",
    "trends_set['nl&ams'] = trending_shared(['Netherlands', 'Amsterdam'])\n",
    "trends_set['ams^nl'] = trending_excl(['Amsterdam', 'Netherlands'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_set['ams^nl']\n",
    "trends_set['ams^nl'][0]\n",
    "trends_set['nl&ams'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trending_shared(['Netherlands', 'Germany'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting search results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the variable `q` to a trending topic, \n",
    "or anything else for that matter. The example query below\n",
    "was a trending topic when this content was being developed\n",
    "and is used throughout the remainder of this chapter\n",
    "\n",
    "[api docs](https://dev.twitter.com/docs/api/1.1/get/search/tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_nodes(x, **kwargs):\n",
    "    \"\"\"\"\"\"\n",
    "    key = kwargs.get('key', None)\n",
    "    y = kwargs.get('y', None)\n",
    "    \n",
    "    # base case\n",
    "    if y is None: y = []\n",
    "    if x == []: return y\n",
    "    \n",
    "    # recursive call\n",
    "    if key is None: \n",
    "        y.append(x[0])\n",
    "#         recursive_nodes(x[1:], None, y)\n",
    "    else: \n",
    "        y.append(x[0][key])\n",
    "    recursive_nodes(x[1:], key=key, y=y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert recursive_nodes([1,2,3,4]) == [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trending_topic(topic, number=100):\n",
    "    \"\"\"Returns status\"\"\"\n",
    "    return twitter_api.search.tweets(q=topic, count=number)['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trending_topic(topic, number=100):\n",
    "    \"\"\"Returns status\"\"\"\n",
    "    return twitter_api.search.tweets(q=topic, count=number)['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trending_text(topic, number=100, head=10):\n",
    "    \"\"\"Returns status text\"\"\"\n",
    "    tweet_ = [(s['user']['screen_name'], recursive_nodes(s['entities']['user_mentions'], key='screen_name'),\n",
    "               recursive_nodes(s['entities']['hashtags'], key='text'), s['text'], s['retweet_count']) \n",
    "              for s in trending_topic(topic)]\n",
    "    return pd.DataFrame(tweet_, columns=['name', 'mentions', 'hashtags', 'text', 'retweets']).sort_values('retweets', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = trends_set['ams^nl'][0]\n",
    "statuses = trending_topic(topic)\n",
    "statuses[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses[0]#['geo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trending_text(topic, number=100, head=10)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mentions[:10]\n",
    "df.hashtags[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting text, screen names, and hashtags from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# status_texts = [status['text'] for status in statuses]\n",
    "\n",
    "# screen_names = [user_mention['screen_name'] for status in statuses\n",
    "#                                             for user_mention in status['entities']['user_mentions']]\n",
    "\n",
    "# hashtags = [hashtag['text'].lower() for status in statuses\n",
    "#                             for hashtag in status['entities']['hashtags']]\n",
    "stopwords = ['de', 'het', 'een', 'is', 'de', 'die', 'dat', 'dit', 'van', 'en', 'rt', 'in', 'er', 'op', 'als', 'aan', 'als', 'bij',\n",
    "             'met', 'niet', 'voor', 'gaat', 'ze', 'je', 'ik', 'wij', 'rt', 'staan', 'kan', 'dan', 'af', 'zoals', 'laat', 'naar',\n",
    "             'meer', 'werd', 'geen', 'na', 'heeft', 'komt', 'wel', 'nog', 'over', '-']\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [w.lower() for t in df['text']\n",
    "           for w in t.split() if w.lower() not in stopwords]\n",
    "Counter(words).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_texts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the first 5 items for each...\n",
    "print('status text: ', json.dumps(status_texts[0:5], indent=1))\n",
    "print('screen names: ', json.dumps(screen_names[0:5], indent=1)) \n",
    "print('hashtags: ', json.dumps(hashtags[0:5], indent=1))\n",
    "print('words: ', json.dumps(words[0:5], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic frequency distribution from the words in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for item in words: #[words, screen_names, hashtags]:\n",
    "    c = Counter(item)\n",
    "    print('-'*80)\n",
    "    print(c.most_common()[:10]) # top 10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Counter(words).most_common(30), columns=['word', 'count']).set_index('word').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Counter(screen_names).most_common(30), columns=['mentions', 'count']).set_index('mentions').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Counter(hashtags).most_common(30), columns=['hashtags', 'count']).set_index('hashtags').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets = [\n",
    "            # Store out a tuple of these three values ...\n",
    "            (status['retweet_count'], \n",
    "             status['retweeted_status']['user']['screen_name'],\n",
    "             status['text'].replace(\"\\n\",\"\\\\\")) \n",
    "            \n",
    "            # ... for each status ...\n",
    "            for status in statuses \n",
    "            \n",
    "            # ... so long as the status meets this condition.\n",
    "                if 'retweeted_status' in status\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retweets = pd.DataFrame(retweets, columns=['retweets', 'screen_name', 'text']).sort_values('retweets', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retweets.text[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
